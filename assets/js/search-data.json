{
  
    
        "post0": {
            "title": "Understanding the losses in CenterNet Architecture",
            "content": "Brief Intro . This is a small demo of the how the ground truths and loss will look in centerNet. Most of the code is from MMdetection. The idea here is to show a demo part so that the code is understandable. My repo for understanding the architecure can be found here . To understand the architecture please go through the blog by Shreejal Trivedi. . How to generate the groundTruth heat maps . We need to generate heatmap, the heatmap is generated such that at the center of the image, where the bounding box is the value will be 1 and decreasing around it like a gaussian. | To calculate the radius we use the bbox height and width which have been scaled down to the feature map level. | We can use the same function used in MMdetection. You can find the detailed description in this link | Lets assume that our initial image was of size (64,64) and it was scaled down to (8,8) after the feature maps and lets assume the radius is 2, in reality the radius is calculated as the function shown below based on the bounding box size | #collapse-hide import torch import numpy as np . . #collapse-hide def gaussian2D(radius, sigma=1, dtype=torch.float32, device=&#39;cpu&#39;): &quot;&quot;&quot;Generate 2D gaussian kernel. Args: radius (int): Radius of gaussian kernel. sigma (int): Sigma of gaussian function. Default: 1. dtype (torch.dtype): Dtype of gaussian tensor. Default: torch.float32. device (str): Device of gaussian tensor. Default: &#39;cpu&#39;. Returns: h (Tensor): Gaussian kernel with a ``(2 * radius + 1) * (2 * radius + 1)`` shape. &quot;&quot;&quot; x = torch.arange( -radius, radius + 1, dtype=dtype, device=device).view(1, -1) y = torch.arange( -radius, radius + 1, dtype=dtype, device=device).view(-1, 1) h = (-(x * x + y * y) / (2 * sigma * sigma)).exp() h[h &lt; torch.finfo(h.dtype).eps * h.max()] = 0 return h def gen_gaussian_target(heatmap, center, radius, k=1): &quot;&quot;&quot;Generate 2D gaussian heatmap. Args: heatmap (Tensor): Input heatmap, the gaussian kernel will cover on it and maintain the max value. center (list[int]): Coord of gaussian kernel&#39;s center. radius (int): Radius of gaussian kernel. k (int): Coefficient of gaussian kernel. Default: 1. Returns: out_heatmap (Tensor): Updated heatmap covered by gaussian kernel. &quot;&quot;&quot; diameter = 2 * radius + 1 gaussian_kernel = gaussian2D( radius, sigma=diameter / 6, dtype=heatmap.dtype, device=heatmap.device) x, y = center height, width = heatmap.shape[:2] left, right = min(x, radius), min(width - x, radius + 1) top, bottom = min(y, radius), min(height - y, radius + 1) masked_heatmap = heatmap[y - top:y + bottom, x - left:x + right] masked_gaussian = gaussian_kernel[radius - top:radius + bottom, radius - left:radius + right] out_heatmap = heatmap torch.max( masked_heatmap, masked_gaussian * k, out=out_heatmap[y - top:y + bottom, x - left:x + right]) return out_heatmap def gaussian_radius(det_size, min_overlap): r&quot;&quot;&quot;Generate 2D gaussian radius. This function is modified from the `official github repo &lt;https://github.com/princeton-vl/CornerNet-Lite/blob/master/core/sample/ utils.py#L65&gt;`_. Given ``min_overlap``, radius could computed by a quadratic equation according to Vieta&#39;s formulas. There are 3 cases for computing gaussian radius, details are following: - Explanation of figure: ``lt`` and ``br`` indicates the left-top and bottom-right corner of ground truth box. ``x`` indicates the generated corner at the limited position when ``radius=r``. - Case1: one corner is inside the gt box and the other is outside. .. code:: text |&lt; width &gt;| lt-+-+ - | | | ^ +--x-+--+ | | | | | | | | height | | overlap | | | | | | | | | | v +--+br--+ - | | | +-+--x To ensure IoU of generated box and gt box is larger than ``min_overlap``: .. math:: cfrac{(w-r)*(h-r)}{w*h+(w+h)r-r^2} ge {iou} quad Rightarrow quad {r^2-(w+h)r+ cfrac{1-iou}{1+iou}*w*h} ge 0 {a} = 1, quad{b} = {-(w+h)}, quad{c} = { cfrac{1-iou}{1+iou}*w*h} {r} le cfrac{-b- sqrt{b^2-4*a*c}}{2*a} - Case2: both two corners are inside the gt box. .. code:: text |&lt; width &gt;| lt-+-+ - | | | ^ +--x-+ | | | | | | |overlap| | height | | | | | +-x--+ | | | v +-+-br - To ensure IoU of generated box and gt box is larger than ``min_overlap``: .. math:: cfrac{(w-2*r)*(h-2*r)}{w*h} ge {iou} quad Rightarrow quad {4r^2-2(w+h)r+(1-iou)*w*h} ge 0 {a} = 4, quad {b} = {-2(w+h)}, quad {c} = {(1-iou)*w*h} {r} le cfrac{-b- sqrt{b^2-4*a*c}}{2*a} - Case3: both two corners are outside the gt box. .. code:: text |&lt; width &gt;| x--+-+ | | | +-lt-+ | - | | | | ^ | | | | | | overlap | | height | | | | | | | | v | +br--+ - | | | +-+--x To ensure IoU of generated box and gt box is larger than ``min_overlap``: .. math:: cfrac{w*h}{(w+2*r)*(h+2*r)} ge {iou} quad Rightarrow quad {4*iou*r^2+2*iou*(w+h)r+(iou-1)*w*h} le 0 {a} = {4*iou}, quad {b} = {2*iou*(w+h)}, quad {c} = {(iou-1)*w*h} {r} le cfrac{-b+ sqrt{b^2-4*a*c}}{2*a} Args: det_size (list[int]): Shape of object. min_overlap (float): Min IoU with ground truth for boxes generated by keypoints inside the gaussian kernel. Returns: radius (int): Radius of gaussian kernel. &quot;&quot;&quot; height, width = det_size a1 = 1 b1 = (height + width) c1 = width * height * (1 - min_overlap) / (1 + min_overlap) sq1 = sqrt(b1**2 - 4 * a1 * c1) r1 = (b1 - sq1) / (2 * a1) a2 = 4 b2 = 2 * (height + width) c2 = (1 - min_overlap) * width * height sq2 = sqrt(b2**2 - 4 * a2 * c2) r2 = (b2 - sq2) / (2 * a2) a3 = 4 * min_overlap b3 = -2 * min_overlap * (height + width) c3 = (min_overlap - 1) * width * height sq3 = sqrt(b3**2 - 4 * a3 * c3) r3 = (b3 + sq3) / (2 * a3) return min(r1, r2, r3) . . h = gaussian2D(radius=2, sigma=1, dtype=torch.float32, device=&#39;cpu&#39;) . h . tensor([[0.0183, 0.0821, 0.1353, 0.0821, 0.0183], [0.0821, 0.3679, 0.6065, 0.3679, 0.0821], [0.1353, 0.6065, 1.0000, 0.6065, 0.1353], [0.0821, 0.3679, 0.6065, 0.3679, 0.0821], [0.0183, 0.0821, 0.1353, 0.0821, 0.0183]]) . Our radius was 2 so we can see that at (2,2) the magnitude is 1 and in a gaussian kernel way,it decreases around.Now this heatmap will be copied to the center as required, but if the center is at the corners then cropping of the heatmap might be requried as needed . As in the begining assume that the heatmap is of shape 8,8 and lets assume that the object is located at the center (3,3). | So we need to copy the ground truth heatMap to that position as is as shown in the code below | radius = 2 heatmap = torch.zeros((8,8)) height, width = heatmap.shape[:2] center=(3,3) x, y = center # we are doing this because the kernel may lie outside the heatmap for example near corners left, right = min(x, radius), min(width - x, radius + 1) top, bottom = min(y, radius), min(height - y, radius + 1) masked_heatmap = heatmap[y - top:y + bottom, x - left:x + right] masked_gaussian = h[radius - top:radius + bottom, radius - left:radius + right] . masked_gaussian . tensor([[0.0183, 0.0821, 0.1353, 0.0821, 0.0183], [0.0821, 0.3679, 0.6065, 0.3679, 0.0821], [0.1353, 0.6065, 1.0000, 0.6065, 0.1353], [0.0821, 0.3679, 0.6065, 0.3679, 0.0821], [0.0183, 0.0821, 0.1353, 0.0821, 0.0183]]) . masked_heatmap . tensor([[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]) . out_heatmap = heatmap torch.max( masked_heatmap, masked_gaussian , out=out_heatmap[y - top:y + bottom, x - left:x + right]) . tensor([[0.0183, 0.0821, 0.1353, 0.0821, 0.0183], [0.0821, 0.3679, 0.6065, 0.3679, 0.0821], [0.1353, 0.6065, 1.0000, 0.6065, 0.1353], [0.0821, 0.3679, 0.6065, 0.3679, 0.0821], [0.0183, 0.0821, 0.1353, 0.0821, 0.0183]]) . out_heatmap . tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0183, 0.0821, 0.1353, 0.0821, 0.0183, 0.0000, 0.0000], [0.0000, 0.0821, 0.3679, 0.6065, 0.3679, 0.0821, 0.0000, 0.0000], [0.0000, 0.1353, 0.6065, 1.0000, 0.6065, 0.1353, 0.0000, 0.0000], [0.0000, 0.0821, 0.3679, 0.6065, 0.3679, 0.0821, 0.0000, 0.0000], [0.0000, 0.0183, 0.0821, 0.1353, 0.0821, 0.0183, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]) . out_heatmap.shape . torch.Size([8, 8]) . We can see that the heatmap has been placed with a value of 1 as (3,3) which is the center we gave. . Now look at how the losses will work. . HeatMap loss . Suppose we have 4 classes, 2 batches and feature map size be 8, then the predicted heatmap will be of shape 2,4,8,8 . (batch,num_classes,height,width) | Suppose we have two images and one bounding box each in two images, let the first image have class id 0 and the second image have classid 2 . So the corresponding depth will have the heatmap gaussian as the ground truth with center having one and the gaussian kernel spread around. | For now we will use the above geneareted heatmap as the object , that is we have object at center 3,3 at id 0 in first image and at id 2 in second image. | groundTruth = torch.zeros((2,4,8,8),dtype=torch.float32) print(&quot;GroundTruth shape&quot;,groundTruth.shape) # now we need to copy the heat map we generated above to the positions of the class ids, # here we have assumed the in the first image the class id 0 is having the bounding box # and in the image 2 the classid 2 is having the object, for simplicity we are assuming # that both the images have same heat map an center, the assignment is as follows then groundTruth[0,0,:,:] = out_heatmap.clone() groundTruth[1,2,:,:] = out_heatmap.clone() print(&quot;Ground Truth n&quot;,groundTruth) . GroundTruth shape torch.Size([2, 4, 8, 8]) Ground Truth tensor([[[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0183, 0.0821, 0.1353, 0.0821, 0.0183, 0.0000, 0.0000], [0.0000, 0.0821, 0.3679, 0.6065, 0.3679, 0.0821, 0.0000, 0.0000], [0.0000, 0.1353, 0.6065, 1.0000, 0.6065, 0.1353, 0.0000, 0.0000], [0.0000, 0.0821, 0.3679, 0.6065, 0.3679, 0.0821, 0.0000, 0.0000], [0.0000, 0.0183, 0.0821, 0.1353, 0.0821, 0.0183, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]], [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0183, 0.0821, 0.1353, 0.0821, 0.0183, 0.0000, 0.0000], [0.0000, 0.0821, 0.3679, 0.6065, 0.3679, 0.0821, 0.0000, 0.0000], [0.0000, 0.1353, 0.6065, 1.0000, 0.6065, 0.1353, 0.0000, 0.0000], [0.0000, 0.0821, 0.3679, 0.6065, 0.3679, 0.0821, 0.0000, 0.0000], [0.0000, 0.0183, 0.0821, 0.1353, 0.0821, 0.0183, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]]) . Now we will make a random prediction and see how we calculate the losses . pred = torch.randn(2,4,8,8,dtype=torch.float32).sigmoid() . # the loss is as the described in the paper and is a modified focal loss alpha = 2.0 gamma = 4.0 eps = 1e-12 pos_weights = groundTruth.eq(1) neg_weigths = (1 - groundTruth).pow(gamma) . From the heat map we can see that the negative samples will be much more and therefore they introduced this modified version of focal loss to counteract that . pos_loss = -(pred+eps).log()*(1 - pred).pow(alpha)*pos_weights neg_loss = -(1 - pred+ eps).log()*pred.pow(alpha)*neg_weigths final_loss = pos_loss + neg_loss print(final_loss.sum()) . tensor(181.3507) . GroundTruth WH and WH offset . The the head predicts the wh in the shape of batch,2,height,widht and the same is the shape of offset head | For wh head the bbox scaled down widht and height will be placed at the indexes 0 and 1. | For offset head the the corressponding difference in offset will be placed at the corresponding indexes. | To understand it better, initially we have assumed that we have an image of shape (64,64) and after scaling down it becomes (8,8). And suppose in the original image the bounding box center is at (28,28) so when we scale it down to feature map level it becomem 28/8 = 3.5 and we have to take the int of that so the offset difference is (3.5 -3) which is 0.5. Its as shown below. | #so cx and cy will be ctx,cty = 28/8 ,28/8 ctx_int,cty_int = int(ctx),int(cty) print(&quot;orginal scaled down &quot;,(ctx, cty) ) print(&quot;floored version &quot; ,(ctx_int,cty_int)) print(&quot;offset is &quot;,(ctx- ctx_int,cty-cty_int)) . orginal scaled down (3.5, 3.5) floored version (3, 3) offset is (0.5, 0.5) . # now we generate the ground truth groundTruthWH =torch.zeros((2,2,8,8),dtype=torch.float32) groundTruthWHOffset = torch.zeros((2,2,8,8),dtype=torch.float32) groundTruthWHOffsetWeights = torch.zeros((2,2,8,8),dtype=torch.float32) . # so for the we have said that the object is same position in both the images, so when # groundTruth is set the to be predicted width and height at the same position groundTruthWHOffset[0,0,ctx_int,cty_int] = ctx- ctx_int groundTruthWHOffset[0,1,ctx_int,cty_int] = cty-cty_int # we are asuming the object is at the same position in both the images so the # above will be the same for batchid 1 groundTruthWHOffset[1,0,ctx_int,cty_int] = ctx- ctx_int groundTruthWHOffset[1,1,ctx_int,cty_int] = cty-cty_int . We need set weights because we need to consider loss only from the places there was an object . groundTruthWHOffsetWeights[0,:,ctx_int,cty_int] = 1 #since the second batch image is also at the same place groundTruthWHOffsetWeights[1,:,ctx_int,cty_int] = 1 . Lets make a random prediction to calculate the loss . predWH = torch.randn((2,2,8,8),dtype=torch.float32) predWHOffset = torch.randn((2,2,8,8),dtype=torch.float32) . The loss we use for the wh and wh_offset are the same and is the l1_loss . def l1_loss(pred, target): &quot;&quot;&quot;L1 loss. Args: pred (torch.Tensor): The prediction. target (torch.Tensor): The learning target of the prediction. Returns: torch.Tensor: Calculated loss &quot;&quot;&quot; if target.numel() == 0: return pred.sum() * 0 assert pred.size() == target.size() loss = torch.abs(pred - target) return loss . #Note we are multiplying by the weights in the end to get the loss from required poistion only WHLoss = l1_loss(predWH,groundTruthWH)*groundTruthWHOffsetWeights WHOffsetLoss = l1_loss(predWHOffset,groundTruthWHOffset)*groundTruthWHOffsetWeights WHLoss = WHLoss.sum() WHOffsetLoss = WHOffsetLoss.sum() . print(&quot;Ground Truth Loss &quot;,WHLoss) print(&quot;Ground Offset Loss &quot;,WHOffsetLoss) . Ground Truth Loss tensor(3.6995) Ground Offset Loss tensor(2.0938) . Final Loss . The final loss is the weighted sum of the heapmap loss, wh loss and wh_offset loss.There is a little more things involved and in the repo i have showed how these are actually done in a real implemenation. Hope this was helpful .",
            "url": "https://akashprakas.github.io/My-blog/jupyter/2022/08/24/Understanding-the-losses-in-CenterNet-Architecture.html",
            "relUrl": "/jupyter/2022/08/24/Understanding-the-losses-in-CenterNet-Architecture.html",
            "date": " • Aug 24, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "So what does does mobile blocks save",
            "content": "Brief Intro . This blog assumes the reader have a some understanding of mobileNet.This is just a lazy illustration of how much different mobileNet block save. The actual papers have the real numbers. If you want to know about the model please go through the papers MobileNetV1 MobileNetV2. In this we will briefly see how much parameters and floating point operations are required by a normal convolution block, mobileNetV1 and mobileNetV2 for the same input size to produce the same output. We will use torchinfo library for getting the summaries. . #collapse-hide import torch import torch.nn as nn from torchinfo import summary import numpy as np . . # we will use the same input and outputs for all the conv blocks and mobile blocks input_filters = 64 output_filters = 128 input_size = (3,input_filters,224,224) . #collapse-hide def printInputAndOutput(model,input_filters=64): rand_tensor = torch.rand((3,input_filters,224,224)) out = model(rand_tensor) print(&quot;Input shape = &quot;, rand_tensor.shape) print(&quot;Output shap =&quot;, out.shape) . . Simple ConvNet Block . simple_convBlock = nn.Sequential(nn.Conv2d(in_channels=input_filters,out_channels=output_filters,kernel_size=3,stride=2, padding=1,bias=False),nn.BatchNorm2d(output_filters), nn.ReLU(inplace=True)) printInputAndOutput(simple_convBlock) . Input shape = torch.Size([3, 64, 224, 224]) Output shap = torch.Size([3, 128, 112, 112]) . summary(simple_convBlock,input_size=input_size,col_names=[&quot;kernel_size&quot;, &quot;output_size&quot;, &quot;num_params&quot;, &quot;mult_adds&quot;]) . ============================================================================================================================================ Layer (type:depth-idx) Kernel Shape Output Shape Param # Mult-Adds ============================================================================================================================================ Sequential -- [3, 128, 112, 112] -- -- ├─Conv2d: 1-1 [3, 3] [3, 128, 112, 112] 73,728 2,774,532,096 ├─BatchNorm2d: 1-2 -- [3, 128, 112, 112] 256 768 ├─ReLU: 1-3 -- [3, 128, 112, 112] -- -- ============================================================================================================================================ Total params: 73,984 Trainable params: 73,984 Non-trainable params: 0 Total mult-adds (G): 2.77 ============================================================================================================================================ Input size (MB): 38.54 Forward/backward pass size (MB): 77.07 Params size (MB): 0.30 Estimated Total Size (MB): 115.90 ============================================================================================================================================ . MobileNet Block . The main idea is to use depth wise convolution to reduce the parameters and floating point operations required. For more info please read the paper or watch this tutorial by Prof Maziar Raissi . mobileNetBlock = nn.Sequential( #DEPTHWISE CONV #we get the depthwise convolution by specifying groups same as in_channels nn.Conv2d(in_channels=input_filters,out_channels=input_filters,kernel_size=3, stride=2,padding=1,groups=input_filters,bias=False), nn.BatchNorm2d(input_filters), nn.ReLU(inplace=True), #POINTWISE CONV nn.Conv2d(in_channels=input_filters,out_channels=output_filters,kernel_size=1, stride=1,padding=0,bias=False), nn.BatchNorm2d(output_filters), nn.ReLU(inplace=True) ) printInputAndOutput(mobileNetBlock) . Input shape = torch.Size([3, 64, 224, 224]) Output shap = torch.Size([3, 128, 112, 112]) . summary(mobileNetBlock,input_size=input_size,col_names=[&quot;kernel_size&quot;, &quot;output_size&quot;, &quot;num_params&quot;, &quot;mult_adds&quot;]) . ============================================================================================================================================ Layer (type:depth-idx) Kernel Shape Output Shape Param # Mult-Adds ============================================================================================================================================ Sequential -- [3, 128, 112, 112] -- -- ├─Conv2d: 1-1 [3, 3] [3, 64, 112, 112] 576 21,676,032 ├─BatchNorm2d: 1-2 -- [3, 64, 112, 112] 128 384 ├─ReLU: 1-3 -- [3, 64, 112, 112] -- -- ├─Conv2d: 1-4 [1, 1] [3, 128, 112, 112] 8,192 308,281,344 ├─BatchNorm2d: 1-5 -- [3, 128, 112, 112] 256 768 ├─ReLU: 1-6 -- [3, 128, 112, 112] -- -- ============================================================================================================================================ Total params: 9,152 Trainable params: 9,152 Non-trainable params: 0 Total mult-adds (M): 329.96 ============================================================================================================================================ Input size (MB): 38.54 Forward/backward pass size (MB): 115.61 Params size (MB): 0.04 Estimated Total Size (MB): 154.18 ============================================================================================================================================ . MobileNetv2 Block . The idea here is to add a residual connection and with this better perfomance was obtained with a slight increase in number of parameters. For more info please read the paper or watch this tutorial by Prof Maziar Raissi . class MobileNetv2Block(nn.Module): def __init__(self,in_channels,out_channels,expand_ratio,stride=1): super(MobileNetv2Block,self).__init__() self.conv1x1Begin = nn.Sequential( nn.Conv2d(in_channels,in_channels*expand_ratio,kernel_size=1,stride=1,bias=False), nn.BatchNorm2d(in_channels*expand_ratio), nn.ReLU6(inplace=True)) self.convDepthWise = nn.Sequential( nn.Conv2d(in_channels*expand_ratio,in_channels*expand_ratio,kernel_size=3,stride=stride,padding=1,groups=in_channels*expand_ratio,bias=False), nn.BatchNorm2d(in_channels*expand_ratio), nn.ReLU6(inplace=True) ) self.conv1x1Last = nn.Sequential( nn.Conv2d(in_channels*expand_ratio,out_channels,kernel_size=1,stride=1,bias=False), nn.BatchNorm2d(out_channels), nn.ReLU6(inplace=True)) self.stride = stride self.use_res_connect = self.stride == 1 and in_channels == out_channels def forward(self,x): input_ = x x = self.conv1x1Begin(x) x = self.convDepthWise(x) x = self.conv1x1Last(x) if self.use_res_connect: return x+input_ else: return x . mobileNetV2Block = MobileNetv2Block(64,128,2,2) printInputAndOutput(mobileNetV2Block) . Input shape = torch.Size([3, 64, 224, 224]) Output shap = torch.Size([3, 128, 112, 112]) . summary(mobileNetV2Block,input_size=input_size,col_names=[&quot;kernel_size&quot;, &quot;output_size&quot;, &quot;num_params&quot;, &quot;mult_adds&quot;]) . ============================================================================================================================================ Layer (type:depth-idx) Kernel Shape Output Shape Param # Mult-Adds ============================================================================================================================================ MobileNetv2Block -- [3, 128, 112, 112] -- -- ├─Sequential: 1-1 -- [3, 128, 224, 224] -- -- │ └─Conv2d: 2-1 [1, 1] [3, 128, 224, 224] 8,192 1,233,125,376 │ └─BatchNorm2d: 2-2 -- [3, 128, 224, 224] 256 768 │ └─ReLU6: 2-3 -- [3, 128, 224, 224] -- -- ├─Sequential: 1-2 -- [3, 128, 112, 112] -- -- │ └─Conv2d: 2-4 [3, 3] [3, 128, 112, 112] 1,152 43,352,064 │ └─BatchNorm2d: 2-5 -- [3, 128, 112, 112] 256 768 │ └─ReLU6: 2-6 -- [3, 128, 112, 112] -- -- ├─Sequential: 1-3 -- [3, 128, 112, 112] -- -- │ └─Conv2d: 2-7 [1, 1] [3, 128, 112, 112] 16,384 616,562,688 │ └─BatchNorm2d: 2-8 -- [3, 128, 112, 112] 256 768 │ └─ReLU6: 2-9 -- [3, 128, 112, 112] -- -- ============================================================================================================================================ Total params: 26,496 Trainable params: 26,496 Non-trainable params: 0 Total mult-adds (G): 1.89 ============================================================================================================================================ Input size (MB): 38.54 Forward/backward pass size (MB): 462.42 Params size (MB): 0.11 Estimated Total Size (MB): 501.06 ============================================================================================================================================ . Comparison . Now we can compare the summaries of each block. From the above cells we can observe that the inputs and output shapes remains the same . 1)SimpleConvBlock . Total params: 73,984 . Trainable params: 73,984 . Non-trainable params: 0 . Total mult-adds (G): 2.77 . 2)MobileNetV1 . Total params: 9,152 . Trainable params: 9,152 . Non-trainable params: 0 . Total mult-adds (M): 329.96 . 3)MobileNetV2 . Total params: 26,496 . Trainable params: 26,496 . Non-trainable params: 0 . Total mult-adds (G): 1.89 . If you look at the outputs of torchinfo you can see that the estimated total size is more for mobileNets than simpleConv block this isbecause we need to store 2 times the intermediate values during training , but this wont be a problem for inference, during inference we only need to store the parameters and architecture, and thus looking above we can see that way fewer parameters and total number of multiplications and additions needed is also low which helps in faster inference. If you want more info please read the papers which are well written. If you want to read about how the torchinfo works please read this blog by Jacob C. Kimmel . Doing all the above with torchvision classes . Actually all the above were taken from torchvision only and we can do the same easily with torchvision classes as shown below . All credits are to the amazing torchvision library . from torchvision.models.mobilenetv2 import MobileNetV2, InvertedResidual,ConvNormActivation . #we have to put the expand_ratio as one which will reduce this to a simple mobilenetV1 block TorchMobileNetV1Block = InvertedResidual(64,128,stride=2,expand_ratio=1) . TorchMobileNetV2Block = InvertedResidual(64,128,stride=2,expand_ratio=2) . printInputAndOutput(TorchMobileNetV1Block) . Input shape = torch.Size([3, 64, 224, 224]) Output shap = torch.Size([3, 128, 112, 112]) . printInputAndOutput(TorchMobileNetV2Block) . Input shape = torch.Size([3, 64, 224, 224]) Output shap = torch.Size([3, 128, 112, 112]) . summary(TorchMobileNetV1Block,input_size=input_size,col_names=[&quot;kernel_size&quot;, &quot;output_size&quot;, &quot;num_params&quot;, &quot;mult_adds&quot;]) . ============================================================================================================================================ Layer (type:depth-idx) Kernel Shape Output Shape Param # Mult-Adds ============================================================================================================================================ InvertedResidual -- [3, 128, 112, 112] -- -- ├─Sequential: 1-1 -- [3, 128, 112, 112] -- -- │ └─ConvNormActivation: 2-1 -- [3, 64, 112, 112] -- -- │ │ └─Conv2d: 3-1 [3, 3] [3, 64, 112, 112] 576 21,676,032 │ │ └─BatchNorm2d: 3-2 -- [3, 64, 112, 112] 128 384 │ │ └─ReLU6: 3-3 -- [3, 64, 112, 112] -- -- │ └─Conv2d: 2-2 [1, 1] [3, 128, 112, 112] 8,192 308,281,344 │ └─BatchNorm2d: 2-3 -- [3, 128, 112, 112] 256 768 ============================================================================================================================================ Total params: 9,152 Trainable params: 9,152 Non-trainable params: 0 Total mult-adds (M): 329.96 ============================================================================================================================================ Input size (MB): 38.54 Forward/backward pass size (MB): 115.61 Params size (MB): 0.04 Estimated Total Size (MB): 154.18 ============================================================================================================================================ . summary(TorchMobileNetV2Block,input_size=input_size,col_names=[&quot;kernel_size&quot;, &quot;output_size&quot;, &quot;num_params&quot;, &quot;mult_adds&quot;]) . ============================================================================================================================================ Layer (type:depth-idx) Kernel Shape Output Shape Param # Mult-Adds ============================================================================================================================================ InvertedResidual -- [3, 128, 112, 112] -- -- ├─Sequential: 1-1 -- [3, 128, 112, 112] -- -- │ └─ConvNormActivation: 2-1 -- [3, 128, 224, 224] -- -- │ │ └─Conv2d: 3-1 [1, 1] [3, 128, 224, 224] 8,192 1,233,125,376 │ │ └─BatchNorm2d: 3-2 -- [3, 128, 224, 224] 256 768 │ │ └─ReLU6: 3-3 -- [3, 128, 224, 224] -- -- │ └─ConvNormActivation: 2-2 -- [3, 128, 112, 112] -- -- │ │ └─Conv2d: 3-4 [3, 3] [3, 128, 112, 112] 1,152 43,352,064 │ │ └─BatchNorm2d: 3-5 -- [3, 128, 112, 112] 256 768 │ │ └─ReLU6: 3-6 -- [3, 128, 112, 112] -- -- │ └─Conv2d: 2-3 [1, 1] [3, 128, 112, 112] 16,384 616,562,688 │ └─BatchNorm2d: 2-4 -- [3, 128, 112, 112] 256 768 ============================================================================================================================================ Total params: 26,496 Trainable params: 26,496 Non-trainable params: 0 Total mult-adds (G): 1.89 ============================================================================================================================================ Input size (MB): 38.54 Forward/backward pass size (MB): 462.42 Params size (MB): 0.11 Estimated Total Size (MB): 501.06 ============================================================================================================================================ .",
            "url": "https://akashprakas.github.io/My-blog/jupyter/2022/07/30/So-what-does-mobile-blocks-save.html",
            "relUrl": "/jupyter/2022/07/30/So-what-does-mobile-blocks-save.html",
            "date": " • Jul 30, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Hacking Into FasterRcnn in Pytorch",
            "content": "Brief Intro . In the post I will show how to tweak some of the internals of FaterRcnn in Pytorch. I am assuming the reader is someone who already have trained an object detection model using pytorch. If not there is and excellent tutorial in pytorch website. . Small Insight into the model . Basically Faster Rcnn is a two stage detector . The first stage is the Region proposal network which is resposible for knowing the objectness and corresponding bounding boxes. So essentially the RegionProposalNetwork will give the proposals of whether and object is there or not | These proposals will be used by the RoIHeads which outputs the detections . Inside the RoIHeads roi align is done | There will be a box head and box predictor | The losses for the predictions | . | In this post i will try to show how we can add custom parts to the torchvision FasterRcnn | #collapse-hide import torch import torchvision from torchvision.models.detection import FasterRCNN from torchvision.models.detection.rpn import AnchorGenerator from torchvision.models.detection.faster_rcnn import FastRCNNPredictor import torch.nn as nn import torch.nn.functional as F print(f&#39;torch version {torch.__version__}&#39;) print(f&#39;torchvision version {torchvision.__version__}&#39;) . . torch version 1.7.0 torchvision version 0.8.1 . Custom Backone . The backbone can be without FeaturePyramidNetwork | With FeaturePyramidNetwork | Custom Backbone without FPN . This is pretty well written in the pytorch tutorials section, i will add some comments to it additionally . backbone = torchvision.models.mobilenet_v2(pretrained=True).features #we need to specify an outchannel of this backone specifically because this outchannel will be #used as an inchannel for the RPNHEAD which is producing the out of RegionProposalNetwork #we can know the number of outchannels by looking into the backbone &quot;backbone??&quot; backbone.out_channels = 1280 #by default the achor generator FasterRcnn assign will be for a FPN backone, so #we need to specify a different anchor generator anchor_generator = AnchorGenerator(sizes=((128, 256, 512),), aspect_ratios=((0.5, 1.0, 2.0),)) #here at each position in the grid there will be 3x3=9 anchors #and if our backbone is not FPN then the forward method will assign the name &#39;0&#39; to feature map #so we need to specify &#39;0 as feature map name&#39; roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[&#39;0&#39;], output_size=9, sampling_ratio=2) #the output size is the output shape of the roi pooled features which will be used by the box head model = FasterRCNN(backbone,num_classes=2,rpn_anchor_generator=anchor_generator) . model.eval() x = [torch.rand(3, 300, 400), torch.rand(3, 500, 600)] predictions = model(x) . Custom Backbone with FPN . The Resnet50Fpn available in torchvision . # load a model pre-trained pre-trained on COCO model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True) # replace the classifier with a new one, that has # num_classes which is user-defined num_classes = 2 # 1 class (person) + background # get number of input features for the classifier in_features = model.roi_heads.box_predictor.cls_score.in_features # replace the pre-trained head with a new one model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) . model.eval() x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)] predictions = model(x) . Adding a different resenet backbone . Just change to a different resenet | Shows how we should change roi_pooler and anchor_generator along with the backbone changes if we are not using all the layers from FPN | Using all layers from FPN . #hte returned layers are layer1,layer2,layer3,layer4 in returned_layers backbone = torchvision.models.detection.backbone_utils.resnet_fpn_backbone(&#39;resnet101&#39;,pretrained=True) model = FasterRCNN(backbone,num_classes=2) . model.eval() x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)] predictions = model(x) . Using not all layers from FPN . The size of the last fature map in a Resnet50.Later i will show the sizes of the feature maps we use when we use FPN. . #collapse-hide #just to show what will be out of of a normal resnet without fpn res = torchvision.models.resnet50() pure = nn.Sequential(*list(res.children())[:-2]) temp = torch.rand(1,3,400,400) pure(temp).shape . . torch.Size([1, 2048, 13, 13]) . The required layers can be obtained by specifying the returned layers parameters.Also the resnet backbone of different depth can be used. . #the returned layers are layer1,layer2,layer3,layer4 in returned_layers backbone = torchvision.models.detection.backbone_utils.resnet_fpn_backbone(&#39;resnet101&#39;,pretrained=True, returned_layers=[2,3,4]) . Here we are using feature maps of the following shapes. . #collapse-hide out = backbone(temp) for i in out.keys(): print(i,&#39; &#39;,out[i].shape) . . 0 torch.Size([1, 256, 50, 50]) 1 torch.Size([1, 256, 25, 25]) 2 torch.Size([1, 256, 13, 13]) pool torch.Size([1, 256, 7, 7]) . #from the above we can see that the feature are feat maps should be 0,1,2,pool #where pool comes from the default extra block roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[&#39;0&#39;,&#39;1&#39;,&#39;2&#39;,&#39;pool&#39;], output_size=7, sampling_ratio=2) . So essentially what we did was we selected the last three layers in FPN by specifying them in the returned layers, by default, the backbone will add a pool layer on top of the last layer. So we are left with four layers. Now the RoIAlign need to be done in these four layers. If we dnt specify the RoIAlign it will use the by default assume we have used all layers from FPN in torchvision. So we need to specifically give the feauture maps that we used. The usage of feature maps can be our application specific, some time you might need to detect small objects sometimes the object of interest will be large objects only. . #we will need to give anchor_generator because the deafault anchor generator assumes we use all layers in fpn #since we have four layers in fpn here we need to specify 4 anchors anchor_sizes = ((32), (64), (128),(256) ) aspect_ratios = ((0.5,1.0, 1.5,2.0,)) * len(anchor_sizes) anchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios) . Since we have four layers in our FPN we need to specify the anchors. So here each feature map will have 4 anchors at each position.So the first feature map will have anchor size 32 and four of them will be there at each position in the feature map of aspect_ratios (0.5,1.0, 1.5,2.0). Now we can pass these to the FasterRCNN class . model = FasterRCNN(backbone,num_classes=2,rpn_anchor_generator=anchor_generator,box_roi_pool=roi_pooler) . model.eval() x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)] predictions = model(x) . Custom Predictor . The predictor is what that outputs the classes and the corresponding bboxes . By default these have two layers one for class and one for bboxes,but we can add more before it if we want to,so if you have a ton of data this might come handy,(remember there is already a box head before the predictor head, so you might not need this) . class Custom_predictor(nn.Module): def __init__(self,in_channels,num_classes): super(Custom_predictor,self).__init__() self.additional_layer = nn.Linear(in_channels,in_channels) #this is the additional layer self.cls_score = nn.Linear(in_channels, num_classes) self.bbox_pred = nn.Linear(in_channels, num_classes * 4) def forward(self,x): if x.dim() == 4: assert list(x.shape[2:]) == [1, 1] x = x.flatten(start_dim=1) x = self.additional_layer(x) scores = self.cls_score(x) bbox_deltas = self.bbox_pred(x) return scores, bbox_deltas . model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True) #we need the out channels of the box head to pass tpp custom predictor in_features = model.roi_heads.box_head.fc7.out_features #now we can add the custom predictor to the model num_classes =2 model.roi_heads.box_predictor = Custom_predictor(in_features,num_classes) . model.eval() x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)] predictions = model(x) . Custom BoxHead . The ouptuts of the roi_align are first passed through the box head before they are passed to the Predictor, there are two linear layers and we can customize them as we want, be careful with the dimensions since they can break the pipeline . model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True) . class CustomHead(nn.Module): def __init__(self,in_channels,roi_outshape,representation_size): super(CustomHead,self).__init__() self.conv = nn.Conv2d(in_channels,in_channels,kernel_size=3,padding=1)#this is teh additional layer adde #we will be sending a flattened layer, the size will eb in_channels*w*h, here roi_outshape represents it self.fc6 = nn.Linear(in_channels*roi_outshape**2, representation_size) self.fc7 = nn.Linear(representation_size, representation_size) def forward(self,x): # breakpoint() x = self.conv(x) x = x.flatten(start_dim=1) import torch.nn.functional as F x = F.relu(self.fc6(x)) x = F.relu(self.fc7(x)) return x . We need in_channels and representation size, remember the output of this is the input of box_predictor, so we can get the representation size of box_head from the input of box_predictor. | The in_channels can be got from the backbone out channels. | After the flattening the width and height also need to be considered which we wil get from roi_pool output. | in_channels = model.backbone.out_channels roi_outshape = model.roi_heads.box_roi_pool.output_size[0] representation_size=model.roi_heads.box_predictor.cls_score.in_features . model.roi_heads.box_head = CustomHead(in_channels,roi_outshape,representation_size) . num_classes=2 model.roi_heads.box_predictor = FastRCNNPredictor(representation_size, num_classes) . model.eval() x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)] predictions = model(x) . CustomLoss Function . This is the modification for loss of FasterRcnn Predictor. . You can modify the loss by defining the fastrcnn_loss and making chages where you want. | Then pass as say model.roi_heads.fastrcnn_loss = Custom_loss | Usually we replace the F.crossentropy loss by say Focal loss or label smoothing loss | import torchvision.models.detection._utils as det_utils import torch.nn.functional as F . The below loss function is taken from Aman Aroras blog. . # Helper functions from fastai def reduce_loss(loss, reduction=&#39;mean&#39;): return loss.mean() if reduction==&#39;mean&#39; else loss.sum() if reduction==&#39;sum&#39; else loss # Implementation from fastai https://github.com/fastai/fastai2/blob/master/fastai2/layers.py#L338 class LabelSmoothingCrossEntropy(nn.Module): def __init__(self, ε:float=0.1, reduction=&#39;mean&#39;): super().__init__() self.ε,self.reduction = ε,reduction def forward(self, output, target): # number of classes c = output.size()[-1] log_preds = F.log_softmax(output, dim=-1) loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction) nll = F.nll_loss(log_preds, target, reduction=self.reduction) # (1-ε)* H(q,p) + ε*H(u,p) return (1-self.ε)*nll + self.ε*(loss/c) . custom_loss = LabelSmoothingCrossEntropy() #torchvision.models.detection.roi_heads.fastrcnn_loss?? . def custom_fastrcnn_loss(class_logits, box_regression, labels, regression_targets): # type: (Tensor, Tensor, List[Tensor], List[Tensor]) -&gt; Tuple[Tensor, Tensor] &quot;&quot;&quot; Computes the loss for Faster R-CNN. Arguments: class_logits (Tensor) box_regression (Tensor) labels (list[BoxList]) regression_targets (Tensor) Returns: classification_loss (Tensor) box_loss (Tensor) &quot;&quot;&quot; labels = torch.cat(labels, dim=0) regression_targets = torch.cat(regression_targets, dim=0) classification_loss = custom_loss(class_logits, labels) #ADDING THE CUSTOM LOSS HERE # get indices that correspond to the regression targets for # the corresponding ground truth labels, to be used with # advanced indexing sampled_pos_inds_subset = torch.where(labels &gt; 0)[0] labels_pos = labels[sampled_pos_inds_subset] N, num_classes = class_logits.shape box_regression = box_regression.reshape(N, -1, 4) box_loss = det_utils.smooth_l1_loss( box_regression[sampled_pos_inds_subset, labels_pos], regression_targets[sampled_pos_inds_subset], beta=1 / 9, size_average=False, ) box_loss = box_loss / labels.numel() return classification_loss, box_loss . Note on how to vary the anchor generator . The way in which anchor generators are assigned when we use backbone with and without fpn is different. When we are not using FPN there will be only one feature map and for that feature map we need to specify anchors of different shapes. . anchor_generator = AnchorGenerator(sizes=((128, 256, 512),), aspect_ratios=((0.5, 1.0, 2.0),)) . In the above case suppose we have a feature map of shape 7x7, then at each cell in it there will be 9 anchors,three each of shapes 128,256 and 512,with the corresponding aspect rations. But when we are using FPN we have different feature maps, so its more effective we use different feature maps for different layers. Small sized objects are deteted using the earlier feature maps and thus for those we can specify a small sized anchor say 32 and for the later layers we can specify larger anchors. . anchor_sizes = ((32), (64), (128),(256) ) aspect_ratios = ((0.5,1.0, 1.5,2.0,)) * len(anchor_sizes) anchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios) . In the above i am using the same aspect ratio for all the sizes so i am just multiplying by the lenght of the anchor_sizes, but if we want to specify different aspect ratios its totally possible. But be carefull to specifiy the same number of aspect ratios for each anchor sizes . Credits . All the above hacks are just modification of the existing wonderful torchvision library. .",
            "url": "https://akashprakas.github.io/My-blog/jupyter/2020/12/19/Hacking_fasterRcnn.html",
            "relUrl": "/jupyter/2020/12/19/Hacking_fasterRcnn.html",
            "date": " • Dec 19, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Explaining IoU",
            "content": "An IOU explanation and implementaion walk through . In this blogpost i will explain what is IOU, where is it used , how is it implemented . What is IOU . IOU is pretty much clear by the name intersection over union. The formula is . IOU = Area of Intersection / Area of union | Area of union = First Box Area + Second Box Area -Intersection Area | . . How is it implemented(basic) . Here i will show a simple implementation in pytorch.If you look at the below picture we will get a basic idea of how to get the intersection between two boxes, the rest are simple . . For the basic implementation of this can be found in this nice blogpost and from that is basic implemenation is like this . #collapse-hide def batch_iou(a, b, epsilon=1e-5): &quot;&quot;&quot; Given two arrays `a` and `b` where each row contains a bounding box defined as a list of four numbers: [x1,y1,x2,y2] where: x1,y1 represent the upper left corner x2,y2 represent the lower right corner It returns the Intersect of Union scores for each corresponding pair of boxes. Args: a: (numpy array) each row containing [x1,y1,x2,y2] coordinates b: (numpy array) each row containing [x1,y1,x2,y2] coordinates epsilon: (float) Small value to prevent division by zero Returns: (numpy array) The Intersect of Union scores for each pair of bounding boxes. &quot;&quot;&quot; # COORDINATES OF THE INTERSECTION BOXES x1 = np.array([a[:, 0], b[:, 0]]).max(axis=0) y1 = np.array([a[:, 1], b[:, 1]]).max(axis=0) x2 = np.array([a[:, 2], b[:, 2]]).min(axis=0) y2 = np.array([a[:, 3], b[:, 3]]).min(axis=0) # AREAS OF OVERLAP - Area where the boxes intersect width = (x2 - x1) height = (y2 - y1) # handle case where there is NO overlap width[width &lt; 0] = 0 height[height &lt; 0] = 0 area_overlap = width * height # COMBINED AREAS area_a = (a[:, 2] - a[:, 0]) * (a[:, 3] - a[:, 1]) area_b = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1]) area_combined = area_a + area_b - area_overlap # RATIO OF AREA OF OVERLAP OVER COMBINED AREA iou = area_overlap / (area_combined + epsilon) return iou . . Where is it used and how to implement for that use case . But the above implementation assumes that both the bounding boxes have the same set of batches,which is rarely the case. IOU is mainly used in object detection tasks. . We will have a set of anchors for each position in the feature map,for eg say if we have a feature map of shape 5x5 and there are 3 anchors per position then there will be 5x5x3=75 total anchors | The Ground trouth boxes for that feature map may be much less the number of anchors | We need to find the matching anchors to the bounding boxes, so we can select that portion of the feature map for the downstream predictions. | Implementing for the above use case . Basically when we get two boxes say . a- B,M,4 -- the anchor boxes after reshaping(B,A,H,W,4) where A is number of anchors . b- B,N,4 --the real bboxes. N is the max number of boxes in certain image and the other images will be padded with -1. . we need to compute iou between a and b so each box in a is compare with each box in b. So we should make N copies of copies of each box in a to be compare with N bboxes. Also if we want to vectorise this operation then we need to make M copies of b. So the final dimensions will be . a - B,M,N,4 b - B,M,N,4 . Now we can say like each slice of the both a and b can be compared . import torch #say the given anchors and bboxes are in shape x_top,y_top,x_btm,y_btm sample_anchors = torch.tensor([[[[[5.,5,15,15], [25,25,35,35],[1,1,9,9]]]]]) #only 1 batch bboxes = torch.tensor([[[1.,1,11,11], [20,20,30,30]]]) B = bboxes.shape[0] no_of_bboxes = bboxes.shape[1] print(&#39;sample anchors n&#39;, sample_anchors,&#39; n&#39;) print(&#39;sample bboxes n&#39;, bboxes,&#39; n&#39;) print(&#39;sample number of anchors shape &#39;,sample_anchors.shape) print(&#39;sample bboxes shape &#39;,bboxes.shape,&#39; n&#39;) . sample anchors tensor([[[[[ 5., 5., 15., 15.], [25., 25., 35., 35.], [ 1., 1., 9., 9.]]]]]) sample bboxes tensor([[[ 1., 1., 11., 11.], [20., 20., 30., 30.]]]) sample number of anchors shape torch.Size([1, 1, 1, 3, 4]) sample bboxes shape torch.Size([1, 2, 4]) . Here we need to compare the 3 anchor boxes with the two bboxes, first we reshape the anchors to be of shape batch,total_anchors,4, . we need to compute iou between sample_anchors and bboxes so each of the 3 anchors are compared with the bboxes which is 2 here. So for vectorized implementation we should make 3 copies of copies of each anchor in sample_anchors to be compare with 2 bboxes. Also if we should make 3 copies of b to aid in vectorized implementation. So the final dimensions will be . sample_anchors - B,3,2,4 | b=boxes - B,3,2,4 | . sample_anchors = sample_anchors.reshape(B,-1,4) no_of_anchors = sample_anchors.shape[1] sample_anchors = sample_anchors.unsqueeze(2).expand(-1,-1,no_of_bboxes,-1) print(sample_anchors) print(sample_anchors.shape) . tensor([[[[ 5., 5., 15., 15.], [ 5., 5., 15., 15.]], [[25., 25., 35., 35.], [25., 25., 35., 35.]], [[ 1., 1., 9., 9.], [ 1., 1., 9., 9.]]]]) torch.Size([1, 3, 2, 4]) . bboxes = bboxes.unsqueeze(1).expand(-1,no_of_anchors,-1,-1) print(bboxes) print(bboxes.shape) . tensor([[[[ 1., 1., 11., 11.], [20., 20., 30., 30.]], [[ 1., 1., 11., 11.], [20., 20., 30., 30.]], [[ 1., 1., 11., 11.], [20., 20., 30., 30.]]]]) torch.Size([1, 3, 2, 4]) . #first we need to find the intersection for that width and height of the intersection area #this inturn can be obtained by finding the lefttop and bottom corner cordinates and subtracting them left_top = torch.max(sample_anchors[:,:,:,:2],bboxes[:,:,:,:2]) right_bottom = torch.min(sample_anchors[:,:,:,2:],bboxes[:,:,:,2:]) delta = right_bottom - left_top print(delta) . tensor([[[[ 6., 6.], [ -5., -5.]], [[-14., -14.], [ 5., 5.]], [[ 8., 8.], [-11., -11.]]]]) . #The first element of delta is width and the next element is height, we can remove negative values #since this will be boxes that are not intersecting #(remember the the image top left if (0,0) and bottom y is positive downwards) delta[delta&lt;0]=0 #now find the intersection area interesection_area = delta[:,:,:,0]*delta[:,:,:,1] print(interesection_area) print(interesection_area.shape) . tensor([[[36., 0.], [ 0., 25.], [64., 0.]]]) torch.Size([1, 3, 2]) . A small picture represntation is tried below,we can see that first and 3rd anchors intersect with first bounding box while the 2nd anchor intersect with the next one . From the intersection area above we can see that the where there are no itersection the area is zero and thus in this case the first and last anchor mathces with the first bbox while the second anchor mathces with the second one . #now we need to find the Area of union which is #Area of union = First Box Area + Second Box Area -Intersection Area sample_anchors_area = (sample_anchors[:,:,:,2]-sample_anchors[:,:,:,0])*(sample_anchors[:,:,:,3] - sample_anchors[:,:,:,1]) bbox_area = (bboxes[:,:,:,2] - bboxes[:,:,:,0]) * (bboxes[:,:,:,3] - bboxes[:,:,:,1]) iou = interesection_area/(sample_anchors_area+bbox_area - interesection_area) print(iou) print(iou.shape) . tensor([[[0.2195, 0.0000], [0.0000, 0.1429], [0.6400, 0.0000]]]) torch.Size([1, 3, 2]) . so the final iou matrix will have shape (Batch,no_of_anchors,no_of_bboxes) . Downstream usage of this iou . This iou matrix will be used for calculation the regression offsets, negative anchors,ground truth class . The other place where iou is used is for mean Average Precision at the end which if possible i will explain in another post . Complete code . Below i will provide a small code for implementing this in a batch . def IOU(anchors,bboxes): #anchors B,A,H,W,4 #bboxes B,N,4 B = anchors.shape[0] anchors = anchors.reshape(B,-1,4) M,N = anchors.shape[1],bboxes.shape[1] #expanding anchors = anchors.unsqueeze(2).expand(-1,-1,N,-1) bboxes = bboxes.unsqueeze(1).expand(-1,M,-1,-1) left_top = torch.max(anchors[:,:,:,:2],bboxes[:,:,:,:2]) right_bottom = torch.min(anchors[:,:,:,2:],bboxes[:,:,:,2:]) delta = right_bottom - left_top delta[delta&lt;0] = 0 intersection_area = delta[:,:,:,0]*delta[:,:,:,1] anchors_area = (anchors[:,:,:,2]-anchors[:,:,:,0])*(anchors[:,:,:,3] -anchors[:,:,:,1]) bbox_area = (bboxes[:,:,:,2] - bboxes[:,:,:,0])* (bboxes[:,:,:,3] - bboxes[:,:,:,1]) iou = interesection_area/(anchors_area+bbox_area - interesection_area) return iou .",
            "url": "https://akashprakas.github.io/My-blog/jupyter/2020/09/20/pytorch_iou.html",
            "relUrl": "/jupyter/2020/09/20/pytorch_iou.html",
            "date": " • Sep 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am an Adas Video Function Developer. My interests are into computer vision (both classical and deep learning), 3D vision, Graphics and Self driving cars. .",
          "url": "https://akashprakas.github.io/My-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}